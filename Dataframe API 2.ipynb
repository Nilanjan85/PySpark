{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e907991-9189-445c-aac2-ebb5bfd995d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3d3fb2-7c84-4e2f-b802-283c4fb0873a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>sal</th><th>dept</th><th>gender</th></tr></thead><tbody><tr><td>1</td><td>Ram</td><td>5000</td><td>it</td><td>male</td></tr><tr><td>2</td><td>Shyam</td><td>8000</td><td>hr</td><td>female</td></tr><tr><td>3</td><td>Jadu</td><td>6000</td><td>it</td><td>male</td></tr><tr><td>4</td><td>Madu</td><td>10000</td><td>it</td><td>female</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Ram",
         "5000",
         "it",
         "male"
        ],
        [
         "2",
         "Shyam",
         "8000",
         "hr",
         "female"
        ],
        [
         "3",
         "Jadu",
         "6000",
         "it",
         "male"
        ],
        [
         "4",
         "Madu",
         "10000",
         "it",
         "female"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sal",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"1\",\"Ram\",\"5000\",\"it\",\"male\"),(\"2\",\"Shyam\",\"8000\",\"hr\",\"female\"),(\"3\",\"Jadu\",\"6000\",\"it\",\"male\"), (\"4\",\"Madu\",\"10000\",\"it\",\"female\")]\n",
    "schemas = [\"id\", \"name\"  ,\"sal\"  , \"dept\"  ,\"gender\" ]\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schemas)\n",
    "df.display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45fdb81-a921-4a1a-ba04-da371b3afe89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+\n|dept|female|male|\n+----+------+----+\n|  hr|     1|null|\n|  it|     1|   2|\n+----+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "### Pivot \n",
    "df.groupBy(\"dept\").pivot(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03ba4ad-f0d3-47a7-a996-30f40330f482",
     "showTitle": true,
     "title": "Collect()"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id 1\nName Ram\nId 2\nName Shyam\nId 3\nName Jadu\nId 4\nName Madu\n"
     ]
    }
   ],
   "source": [
    "for i in df.collect():\n",
    "    print(\"Id \"+i[0])\n",
    "    print(\"Name \"+i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ed2c8d-9491-4d24-bdbf-7ed2100519ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: list"
     ]
    }
   ],
   "source": [
    "type(df.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e1a546e-83e1-4134-827f-c9e9c116c9f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09f0457-8efa-405b-92dc-7cd35ac7628a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----+------+\n| id| name|  sal|dept|gender|\n+---+-----+-----+----+------+\n|  1|  Ram| 5000|  it|  male|\n|  2|Shyam| 8000|  hr|female|\n|  3| Jadu| 6000|  it|  male|\n|  4| Madu|10000|  it|female|\n+---+-----+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"1\",\"Ram\",\"5000\",\"it\",\"male\"),(\"2\",\"Shyam\",\"8000\",\"hr\",\"female\"),(\"3\",\"Jadu\",\"6000\",\"it\",\"male\"), (\"4\",\"Madu\",\"10000\",\"it\",\"female\")]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(data)\n",
    "type(rdd1)\n",
    "\n",
    "df1=rdd1.toDF([\"id\", \"name\"  ,\"sal\"  , \"dept\"  ,\"gender\" ])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eabc30b4-1954-4986-8e78-ed509c3393d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "creating rdd from a python collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17fcd03-a1a5-4939-a353-ae476d3e94d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: ['Ram', 'Shyam', 'Jadu', 'Madu']"
     ]
    }
   ],
   "source": [
    "rdd2 = spark.sparkContext.parallelize(df.collect())\n",
    "rdd3 = rdd2.map(lambda x : x[1])\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82effb9-9a0e-4b3b-af8f-052e8f446675",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ram', 'saran', 'ram'), ('shyam', 'gupta', 'shyam')]\n"
     ]
    }
   ],
   "source": [
    "### Map function\n",
    "\n",
    "data = [(\"ram\",\"saran\"),(\"shyam\",\"gupta\")]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2 = rdd1.map(lambda x: x+(x[0],))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b898b757-520c-4a69-812d-fa1b9a661976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ram', 'saran'], ['shyam', 'gupta']]\n"
     ]
    }
   ],
   "source": [
    "###  Map function\n",
    "\n",
    "data = [(\"ram saran\"),(\"shyam gupta\")]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2 = rdd1.map(lambda x: x.split(' '))\n",
    "print(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26e238d7-8545-4a4f-b397-f55e685f7690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ram', 'saran', 'shyam', 'gupta']\n"
     ]
    }
   ],
   "source": [
    "### Flat Map function\n",
    "\n",
    "data = [(\"ram saran\"),(\"shyam gupta\")]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2 = rdd1.flatMap(lambda x: x.split(' '))\n",
    "print(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbe9c01-4665-458c-9243-0391ccb3efaf",
     "showTitle": true,
     "title": "List/Array Data type to Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n| name|    marks|\n+-----+---------+\n|  ram| [10, 20]|\n|shyam| [60, 70]|\n| jadu| [20, 50]|\n|madhu|[100, 40]|\n+-----+---------+\n\nroot\n |-- name: string (nullable = true)\n |-- marks: array (nullable = true)\n |    |-- element: long (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"ram\",[10,20]),(\"shyam\",[60,70]),(\"jadu\",[20,50]),(\"madhu\",[100,40])]\n",
    "schemas = [\"name\",\"marks\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schemas)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f481b1c-f4c8-49ff-9e11-0d2086f07c42",
     "showTitle": true,
     "title": "Map DataType to Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n| name|          properties|\n+-----+--------------------+\n|  ram|{eye -> black, ha...|\n|shyam|{eye -> black, ha...|\n| jadu|{eye -> black, ha...|\n|madhu|{eye -> black, ha...|\n+-----+--------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"ram\",{\"hair\":\"black\",\"eye\":\"black\"}),(\"shyam\",{\"hair\":\"black\",\"eye\":\"black\"}),(\"jadu\",{\"hair\":\"black\",\"eye\":\"black\"}),(\"madhu\",{\"hair\":\"black\",\"eye\":\"black\"})]\n",
    "\n",
    "schemas = [\"name\",\"properties\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schemas)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6afc64-2f1a-445b-9651-1e09a097e9a2",
     "showTitle": true,
     "title": "Nested Json "
    }
   },
   "outputs": [],
   "source": [
    "#read the jason file \n",
    "rest_json = spark.read.format(\"json\").option(\"multiLine\",True).load(\"/FileStore/tables/file1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c86dc89-61bb-4547-a226-043bf6c47fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n |-- new_resturant: struct (nullable = true)\n |    |-- restaurant: struct (nullable = true)\n |    |    |-- R: struct (nullable = true)\n |    |    |    |-- res_id: long (nullable = true)\n |    |    |-- apikey: string (nullable = true)\n |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |-- book_url: string (nullable = true)\n |    |    |-- cuisines: string (nullable = true)\n |    |    |-- currency: string (nullable = true)\n |    |    |-- deeplink: string (nullable = true)\n |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- featured_image: string (nullable = true)\n |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |-- has_table_booking: long (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |-- location: struct (nullable = true)\n |    |    |    |-- address: string (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |-- locality: string (nullable = true)\n |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |-- zipcode: string (nullable = true)\n |    |    |-- menu_url: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- offers: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- offer: struct (nullable = true)\n |    |    |    |    |    |-- added_by: long (nullable = true)\n |    |    |    |    |    |-- applicable_on: long (nullable = true)\n |    |    |    |    |    |-- date_added: string (nullable = true)\n |    |    |    |    |    |-- disclaimer: string (nullable = true)\n |    |    |    |    |    |-- end_date: string (nullable = true)\n |    |    |    |    |    |-- friendly_end_date: string (nullable = true)\n |    |    |    |    |    |-- friendly_start_date: string (nullable = true)\n |    |    |    |    |    |-- impressions: long (nullable = true)\n |    |    |    |    |    |-- is_active: long (nullable = true)\n |    |    |    |    |    |-- is_editable: long (nullable = true)\n |    |    |    |    |    |-- is_valid: long (nullable = true)\n |    |    |    |    |    |-- offer_id: long (nullable = true)\n |    |    |    |    |    |-- offer_text: string (nullable = true)\n |    |    |    |    |    |-- offer_type: string (nullable = true)\n |    |    |    |    |    |-- restaurant_list: array (nullable = true)\n |    |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |    |-- restaurants: array (nullable = true)\n |    |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |    |-- share_url: string (nullable = true)\n |    |    |    |    |    |-- start_date: string (nullable = true)\n |    |    |    |    |    |-- status: long (nullable = true)\n |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |-- type_code: long (nullable = true)\n |    |    |    |    |    |-- voucher_id: long (nullable = true)\n |    |    |-- order_deeplink: string (nullable = true)\n |    |    |-- order_url: string (nullable = true)\n |    |    |-- photos_url: string (nullable = true)\n |    |    |-- price_range: long (nullable = true)\n |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |-- thumb: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |-- votes: string (nullable = true)\n |    |    |-- zomato_events: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- event: struct (nullable = true)\n |    |    |    |    |    |-- book_link: string (nullable = true)\n |    |    |    |    |    |-- date_added: string (nullable = true)\n |    |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |    |-- disclaimer: string (nullable = true)\n |    |    |    |    |    |-- display_date: string (nullable = true)\n |    |    |    |    |    |-- display_time: string (nullable = true)\n |    |    |    |    |    |-- end_date: string (nullable = true)\n |    |    |    |    |    |-- end_time: string (nullable = true)\n |    |    |    |    |    |-- event_category: long (nullable = true)\n |    |    |    |    |    |-- event_category_name: string (nullable = true)\n |    |    |    |    |    |-- event_id: long (nullable = true)\n |    |    |    |    |    |-- friendly_end_date: string (nullable = true)\n |    |    |    |    |    |-- friendly_start_date: string (nullable = true)\n |    |    |    |    |    |-- is_active: long (nullable = true)\n |    |    |    |    |    |-- is_end_time_set: long (nullable = true)\n |    |    |    |    |    |-- is_valid: long (nullable = true)\n |    |    |    |    |    |-- photos: array (nullable = true)\n |    |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |    |-- photo: struct (nullable = true)\n |    |    |    |    |    |    |    |    |-- md5sum: string (nullable = true)\n |    |    |    |    |    |    |    |    |-- order: long (nullable = true)\n |    |    |    |    |    |    |    |    |-- photo_id: long (nullable = true)\n |    |    |    |    |    |    |    |    |-- thumb_url: string (nullable = true)\n |    |    |    |    |    |    |    |    |-- type: string (nullable = true)\n |    |    |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |    |    |-- uuid: long (nullable = true)\n |    |    |    |    |    |-- restaurants: array (nullable = true)\n |    |    |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |    |    |-- share_url: string (nullable = true)\n |    |    |    |    |    |-- start_date: string (nullable = true)\n |    |    |    |    |    |-- start_time: string (nullable = true)\n |    |    |    |    |    |-- title: string (nullable = true)\n |-- res_id: long (nullable = true)\n |-- new_establishment: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "rest_json.select(\"*\",explode(\"restaurants\").alias(\"new_resturant\"),\"new_resturant.restaurant.R.res_id\") \\\n",
    ".select(\"*\",explode(\"new_resturant.restaurant.establishment_types\").alias(\"new_establishment\")).drop(\"restaurants\").printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233f887a-350c-42c0-af56-819dfdfcc044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n\njson(path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads JSON files and returns the results as a :class:`DataFrame`.\n    \n    `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n    \n    If the ``schema`` parameter is not specified, this function goes\n    through the input once to determine the input schema.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    path : str, list or :class:`RDD`\n        string represents path to the JSON dataset, or a list of paths,\n        or RDD of Strings storing JSON objects.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema or\n        a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a JSON file and read it back.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a JSON file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n    ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n    ...\n    ...     # Read the JSON file as a DataFrame.\n    ...     spark.read.json(d).show()\n    +---+------------+\n    |age|        name|\n    +---+------------+\n    |100|Hyukjin Kwon|\n    +---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dataframe API 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
